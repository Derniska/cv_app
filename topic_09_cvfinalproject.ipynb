{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7c5d7c8-aa87-44a2-8d94-2426e8f2466a",
   "metadata": {},
   "source": [
    "# Deep Learning for Business Applications course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f62655-2e46-4de8-95ed-ceb1f578baba",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TOPIC 5: Object detection problem. YOLO training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73aa981-0f41-4a8e-b523-0d3e3d4b761b",
   "metadata": {},
   "source": [
    "### 1. Libraries and configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa90d67-0ed9-4eae-a585-f61fc22c09ee",
   "metadata": {},
   "source": [
    "#### 1.1. Streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04217bb0-3144-443f-a9f6-c7f92a7d26b9",
   "metadata": {},
   "source": [
    "[Streamlit](https://streamlit.io/) is a framework that offers a faster way to build and share data applications. It helps you to turn data scripts into shareable web apps in minutes. It is written in pure Python and does not require front‑end experience to work with. Installation is very simple in our environment. Just use terminal or type here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "908bb1f7-311f-4021-8540-ac1b4da2694a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install streamlit -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34823bc0-94e4-44b9-a0d7-96702d08e99e",
   "metadata": {},
   "source": [
    "#### 1.2. Other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811715af-3719-4d9f-84ab-2792d0ecba7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06d0f6c8-b2a9-46a6-8055-7524703672c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.20.0 requires ml_dtypes<1.0.0,>=0.5.1, but you have ml-dtypes 0.3.2 which is incompatible.\n",
      "tensorflow 2.20.0 requires protobuf>=5.28.0, but you have protobuf 4.25.8 which is incompatible.\n",
      "tensorflow 2.20.0 requires tensorboard~=2.20.0, but you have tensorboard 2.16.2 which is incompatible.\n",
      "onnx 1.19.1 requires ml_dtypes>=0.5.0, but you have ml-dtypes 0.3.2 which is incompatible.\n",
      "grpcio-tools 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
      "opentelemetry-proto 1.38.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\n",
      "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
      "chromadb 0.5.23 requires tokenizers<=0.20.3,>=0.13.2, but you have tokenizers 0.22.1 which is incompatible.\n",
      "yandexcloud 0.369.0 requires protobuf<6,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-cpu==2.16.1 -q\n",
    "!pip install tf-keras==2.16.0 --no-dependencies -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "992d5c84-921a-43ee-b5da-5f1204d2b75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d390f276-4198-42cd-aa1c-d3c7a70d0f7f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ultralytics -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "651701ae-d261-45e0-bd7e-45906b293e9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "grpcio-tools 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.33.1 which is incompatible.\n",
      "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.33.1 which is incompatible.\n",
      "yandexcloud 0.369.0 requires protobuf<6,>=5.29.1, but you have protobuf 6.33.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U tensorflow-cpu tf-keras protobuf -q\n",
    "!pip install deepface==0.0.95 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d80cba5-41d0-4279-b266-1dd16b663878",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59de48cf-d797-4cae-819c-69a586b839ee",
   "metadata": {},
   "source": [
    "#### 1.3. Free space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "463e25f1-3fa9-40c1-b59d-b7ac662dce5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmpfs                                      64M     0   64M   0% /dev\n",
      "/dev/vdf                                   12G  9.1G  2.7G  78% /home/jovyan\n",
      "/dev/vda2                                 126G   98G   24G  81% /etc/hosts\n",
      "shm                                        64M     0   64M   0% /dev/shm\n"
     ]
    }
   ],
   "source": [
    "!df -h | grep dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0334c25-dfff-460b-b521-46dd8952b3be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8\n",
      "4 drwxrwsr-x 6 jovyan users 4096 Oct 28 14:46 models--Salesforce--blip-image-captioning-base\n",
      "4 drwxrwsr-x 6 jovyan users 4096 Oct 28 12:26 models--openai--clip-vit-base-patch16\n",
      "total 0\n",
      "total 8\n",
      "drwxrwsr-x 2 jovyan users 4096 Oct 28 12:26 .\n",
      "drwxrwsr-x 3 jovyan users 4096 Oct 28 12:26 ..\n"
     ]
    }
   ],
   "source": [
    "# a cache dir for Huggin Face Hub models\n",
    "!ls -ls ~/.cache/huggingface/hub\n",
    "\n",
    "# a cache dir for PyTorch models\n",
    "!ls -ls ~/.cache/torch/hub/\n",
    "\n",
    "# a cache dir for DeepFace models\n",
    "!ls -la ~/.deepface/weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "719e8946-ab4e-411c-a0e9-bbc90d29695a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use `rm -rf` !!! WITH CARE !!!\n",
    "\n",
    "!rm -rf ~/.cache/huggingface/hub\n",
    "!rm -rf ~/.cache/torch/hub/checkpoints\n",
    "!rm -rf ~/.deepface/weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ca741a2-d3bb-4b15-9ae0-6b0c392b979d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmpfs                                      64M     0   64M   0% /dev\n",
      "/dev/vdb                                   12G  7.0G  4.8G  60% /home/jovyan\n",
      "/dev/vda2                                 126G   56G   66G  46% /etc/hosts\n",
      "shm                                        64M  4.0K   64M   1% /dev/shm\n"
     ]
    }
   ],
   "source": [
    "!df -h | grep dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2d1367-0a9e-4f7c-bb8f-b798ce52d5a6",
   "metadata": {},
   "source": [
    "### 2. How Streamlit works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cb7058-ad37-458b-b278-044806419a31",
   "metadata": {
    "tags": []
   },
   "source": [
    "[Main concepts](https://docs.streamlit.io/library/get-started/main-concepts) require you to create a normal Python script with all necessary elements for your future app and run it with `streamlit run` like `streamlit run your_script.py [-- script args]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80e1028-ada6-46e6-85e9-002086c6f6f7",
   "metadata": {},
   "source": [
    "#### 2.1. Python script with app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fa5935-7b74-4b17-b664-7113b6d8a469",
   "metadata": {},
   "source": [
    "Streamlit's architecture allows you to write apps the same way you write plain Python scripts. Let's create the sample script with `%%writefile` magic command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f3877b6c-2817-4f68-8825-419a8f8a99df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stapp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile stapp.py\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "# Title of our demo app\n",
    "st.title('Meet the first Streamlit application')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74095332-886b-4cda-8169-a0642cd53aab",
   "metadata": {},
   "source": [
    "#### 2.2. Run application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491d23de-7527-44bb-a4ff-a505f7fa7acb",
   "metadata": {},
   "source": [
    "Run application is very easy. Just open a terminal in the folder with your Python script `stapp.py` and type:\n",
    "\n",
    "`streamlit run stapp.py --server.port 20000 --browser.gatherUsageStats False` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8951f3ce-28a5-45a9-ac31-e04fbbe002a5",
   "metadata": {},
   "source": [
    "Your Streamlit application will be available with the following URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8d15b55-a38a-4888-90d8-b0b26aa99f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streamlit available at: https://jhas01.gsom.spbu.ru/user/st133843/proxy/20001/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print('Streamlit available at:',\n",
    "      'https://jhas01.gsom.spbu.ru{}proxy/{}/'.format(\n",
    "          os.environ['JUPYTERHUB_SERVICE_PREFIX'], 20001))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6498d5c1-a3fb-4050-bc78-b102d8668511",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 2.2. Basic examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701c63dd-c203-4952-ad67-f29d478a9d43",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### 2.2.1. Nice headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8fd66dbb-36fa-433d-829c-9a87340dd160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stapp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile stapp.py\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "st.header('Nice looking header string', divider='rainbow')\n",
    "st.header('_Here is header under the line_ :fire:')\n",
    "\n",
    "st.subheader('Subheader is also here', divider='rainbow')\n",
    "st.subheader(':blue[_We like Streamlit_] :star:')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eb65cc-d528-48fe-9905-5eab89d38ab8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### 2.2.2. Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b334000c-10bb-449c-a2f7-9b1bdf6dda5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stapp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile stapp.py\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "st.header('Just a header', divider='rainbow')\n",
    "st.text('Just a text under the header')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2f29cf-9af2-4687-a3fd-8639e809c3f4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### 2.2.3. Write"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d83f88-dad4-477a-a817-23bf578f7fe3",
   "metadata": {},
   "source": [
    "Along with magic commands, `st.write()` is Streamlit's \"Swiss Army knife\". You can pass almost anything to `st.write()`: text, data, Matplotlib figures, charts and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "536fbd00-8d22-4b2e-9adc-e5b72090108a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stapp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile stapp.py\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import streamlit as st\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "st.header('Demo of write function', divider='rainbow')\n",
    "st.subheader('Table and plot at one application')\n",
    "\n",
    "st.divider()\n",
    "\n",
    "st.write(\"Here's demo table from the dataframe:\")\n",
    "fruits_data = pd.DataFrame(\n",
    "    {\n",
    "        'fruits': ['apple', 'peach', 'pineapple', 'watermelon'],\n",
    "        'color': ['green', 'orange', 'yellow', 'stripes'],\n",
    "        'weight': [1, 2, 5, 10]\n",
    "    }\n",
    ")\n",
    "st.write(fruits_data)\n",
    "\n",
    "st.divider()\n",
    "\n",
    "st.write(\"Here's demo chart for fruits:\")\n",
    "chart_data = pd.DataFrame(\n",
    "     np.random.randn(20, 4),\n",
    "     columns=['apple', 'peach', 'pineapple', 'watermelon']\n",
    ")\n",
    "st.line_chart(chart_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ccde3d-f9a5-4ac6-bbdd-faa65c5d82b9",
   "metadata": {},
   "source": [
    "### 3. AI with Streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a959f5da-09e9-4109-99fa-db3da3ac1407",
   "metadata": {},
   "source": [
    "#### 3.1. Upload pipeline for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4e80d0fa-f4d0-4da9-8ce2-0989032f70b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stapp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile stapp.py\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import io\n",
    "import streamlit as st\n",
    "from PIL import Image\n",
    "\n",
    "st.header('Demo of image uploading', divider='rainbow')\n",
    "st.subheader('Uploading file and plot image')\n",
    "st.divider()\n",
    "\n",
    "st.write('#### Upload you image')\n",
    "uploaded_file = st.file_uploader('Select an image file (JPEG format)')\n",
    "if uploaded_file is not None:\n",
    "    file_name = uploaded_file.name\n",
    "    if '.jpg' in file_name:\n",
    "        bytes_data = uploaded_file.read()\n",
    "        img = Image.open(io.BytesIO(bytes_data))\n",
    "        st.divider()\n",
    "        st.image(img, caption='Uploaded image')\n",
    "    else:\n",
    "        st.error('File read error', icon='⚠️')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f889616e-5f2e-4ff2-8a92-c9928b93ceb0",
   "metadata": {},
   "source": [
    "#### 3.2. Add some AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6a2510ba-77f7-4872-96e4-51765bd3bcf4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stapp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile stapp.py\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# use of AI model for image captioning\n",
    "\n",
    "import io\n",
    "import streamlit as st\n",
    "from PIL import Image\n",
    "from transformers import (\n",
    "    BlipProcessor, \n",
    "    BlipForConditionalGeneration\n",
    ")\n",
    "\n",
    "\n",
    "def img_caption(model, processor, img, text=None):\n",
    "    \"\"\"\n",
    "    Uses BLIP model to caption image.\n",
    "    \n",
    "    \"\"\"\n",
    "    res = None\n",
    "    if text:\n",
    "        # conditional image captioning\n",
    "        inputs = processor(img, text, return_tensors='pt')\n",
    "    else:\n",
    "        # unconditional image captioning\n",
    "        inputs = processor(img, return_tensors='pt')\n",
    "    out = model.generate(**inputs)\n",
    "    res = processor.decode(out[0], skip_special_tokens=True)\n",
    "    return res\n",
    "\n",
    "\n",
    "with st.spinner('Please wait, application is initializing...'):\n",
    "    MODEL_CAP_NAME = 'Salesforce/blip-image-captioning-base'\n",
    "    PROCESSOR_CAP = BlipProcessor.from_pretrained(MODEL_CAP_NAME)\n",
    "    MODEL_CAP = BlipForConditionalGeneration.from_pretrained(MODEL_CAP_NAME)\n",
    "\n",
    "st.header('Demo of image uploading', divider='rainbow')\n",
    "st.subheader('Uploading file and plot image with AI caption')\n",
    "st.divider()\n",
    "\n",
    "st.write('#### Upload you image')\n",
    "uploaded_file = st.file_uploader('Select an image file (JPEG format)')\n",
    "if uploaded_file is not None:\n",
    "    file_name = uploaded_file.name\n",
    "    if '.jpg' in file_name:\n",
    "        # input text for conditional image captioning\n",
    "        text = st.text_input(\n",
    "            'Input text for conditional image captioning (if needed)', \n",
    "            ''\n",
    "        )\n",
    "        with st.spinner('Please wait...'):\n",
    "            bytes_data = uploaded_file.read()\n",
    "            img = Image.open(io.BytesIO(bytes_data))\n",
    "            \n",
    "            # use image caption model for uploaded image\n",
    "            caption = img_caption(\n",
    "                model=MODEL_CAP, \n",
    "                processor=PROCESSOR_CAP, \n",
    "                img=img, \n",
    "                text=text\n",
    "            )\n",
    "            st.divider()\n",
    "            st.image(img, caption=caption)\n",
    "    else:\n",
    "        st.error('File read error', icon='⚠️')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "83b0a903-603a-4e72-bb05-f687aee8fef7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stapp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile stapp.py\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# Use of AI model for image captioning\n",
    "# and object detection at the image\n",
    "\n",
    "import io\n",
    "import streamlit as st\n",
    "from PIL import Image\n",
    "from transformers import (\n",
    "    BlipProcessor, \n",
    "    BlipForConditionalGeneration\n",
    ")\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "def img_caption(model, processor, img, text=None):\n",
    "    \"\"\"\n",
    "    Uses BLIP model to caption image.\n",
    "    \n",
    "    \"\"\"\n",
    "    res = None\n",
    "    if text:\n",
    "        # conditional image captioning\n",
    "        inputs = processor(img, text, return_tensors='pt')\n",
    "    else:\n",
    "        # unconditional image captioning\n",
    "        inputs = processor(img, return_tensors='pt')\n",
    "    out = model.generate(**inputs)\n",
    "    res = processor.decode(out[0], skip_special_tokens=True)\n",
    "    return res\n",
    "\n",
    "\n",
    "def img_detect(model, img, plot=False):\n",
    "    \"\"\"\n",
    "    Run YOLO inference on an image.\n",
    "    \n",
    "    \"\"\"\n",
    "    result = model(img)[0]\n",
    "    boxes = result.boxes  # boxes object for bounding box outputs\n",
    "    names = model.names\n",
    "    objs = []\n",
    "    for c, p in zip(boxes.cls, boxes.conf):\n",
    "        objs.append({names[int(c)]: p.item()})\n",
    "    img_bgr = result.plot()  # BGR-order numpy array\n",
    "    img_rgb = Image.fromarray(img_bgr[..., ::-1])  # RGB-order PIL image\n",
    "    if plot:\n",
    "        plt.figure(figsize=(16, 8))\n",
    "        plt.imshow(img_rgb)\n",
    "        plt.show()\n",
    "    return objs, img_rgb\n",
    "\n",
    "\n",
    "with st.spinner('Please wait, application is initializing...'):\n",
    "    MODEL_CAP_NAME = 'Salesforce/blip-image-captioning-base'\n",
    "    PROCESSOR_CAP = BlipProcessor.from_pretrained(MODEL_CAP_NAME)\n",
    "    MODEL_CAP = BlipForConditionalGeneration.from_pretrained(MODEL_CAP_NAME)\n",
    "\n",
    "    MODEL_DET_NAME = 'yolov8n.pt'\n",
    "    MODEL_DET = YOLO(MODEL_DET_NAME)\n",
    "\n",
    "st.header('Demo of image uploading', divider='rainbow')\n",
    "st.subheader('Uploading file and plot image with AI caption and YOLO detection')\n",
    "st.divider()\n",
    "\n",
    "st.write('#### Upload you image')\n",
    "uploaded_file = st.file_uploader('Select an image file (JPEG format)')\n",
    "if uploaded_file is not None:\n",
    "    file_name = uploaded_file.name\n",
    "    if '.jpg' in file_name:\n",
    "        # input text for conditional image captioning\n",
    "        text = st.text_input(\n",
    "            'Input text for conditional image captioning (if needed)', \n",
    "            ''\n",
    "        )\n",
    "        with st.spinner('Please wait...'):\n",
    "            bytes_data = uploaded_file.read()\n",
    "            img = Image.open(io.BytesIO(bytes_data))\n",
    "            \n",
    "            # image caption model for uploaded image\n",
    "            caption = img_caption(\n",
    "                model=MODEL_CAP, \n",
    "                processor=PROCESSOR_CAP, \n",
    "                img=img, \n",
    "                text=text\n",
    "            )\n",
    "            st.divider()\n",
    "            st.image(img, caption=caption)\n",
    "            \n",
    "            # object detection model for uploaded image\n",
    "            objs, img_det = img_detect(\n",
    "                model=MODEL_DET, \n",
    "                img=img\n",
    "            )\n",
    "            st.divider()\n",
    "            st.image(img_det, caption='object detection', width=800)\n",
    "            st.divider()\n",
    "            st.caption('Objects dictionary:')\n",
    "            st.write(objs)\n",
    "    else:\n",
    "        st.error('File read error', icon='⚠️')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d9692f84-e389-44e2-b325-be19fd9cb045",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stapp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile stapp.py\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# Use of AI model for image captioning\n",
    "# and object detection at the image\n",
    "\n",
    "import io\n",
    "import streamlit as st\n",
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def zeroshot(classifier, classes, img):\n",
    "    scores = classifier(\n",
    "        img,\n",
    "        candidate_labels=classes\n",
    "    )\n",
    "    return scores\n",
    "\n",
    "\n",
    "with st.spinner('Please wait, application is initializing...'):\n",
    "    MODEL_ZERO_NAME = 'openai/clip-vit-base-patch16'\n",
    "    CLASSIFIER_ZERO = pipeline('zero-shot-image-classification', model=MODEL_ZERO_NAME)\n",
    "    CLASSES = [\n",
    "        'a photo of nature',\n",
    "        'a photo of cat',\n",
    "        'a photo of a party',\n",
    "        'a photo of a food'\n",
    "    ]\n",
    "\n",
    "st.header('Demo of image uploading', divider='rainbow')\n",
    "st.subheader('Uploading file and classifying it with zero-shot')\n",
    "st.divider()\n",
    "\n",
    "st.write('#### Upload you image')\n",
    "uploaded_file = st.file_uploader('Select an image file (JPEG format)')\n",
    "if uploaded_file is not None:\n",
    "    file_name = uploaded_file.name\n",
    "    if '.jpg' in file_name:\n",
    "        with st.spinner('Please wait...'):\n",
    "            bytes_data = uploaded_file.read()\n",
    "            img = Image.open(io.BytesIO(bytes_data))\n",
    "            \n",
    "            # classifying image with zero-shot modele\n",
    "            scores = zeroshot(\n",
    "                classifier=CLASSIFIER_ZERO, \n",
    "                classes=CLASSES, \n",
    "                img=img\n",
    "            )\n",
    "            st.divider()\n",
    "            st.image(img, caption='zero-shot classification')\n",
    "            \n",
    "            # plot a diagram ith scores and scores output\n",
    "            st.divider()\n",
    "            df = pd.DataFrame(scores)\n",
    "            df = df.set_index('label')\n",
    "            st.bar_chart(df)\n",
    "            st.divider()\n",
    "            st.caption('Scores dictionary:')\n",
    "            st.write(scores)\n",
    "    else:\n",
    "        st.error('File read error', icon='⚠️')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b5a314-6294-4005-8957-b8de29984ece",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3.3. Add some OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "78889b0e-e7a7-47a2-bc7b-450217f431f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stapp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile stapp.py\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# Use of OCR model for text extracting \n",
    "# from image or PDF file\n",
    "\n",
    "import io\n",
    "import streamlit as st\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_bytes\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def pdf2img(pdf_bytes):\n",
    "    \"\"\"\n",
    "    Turns pdf file to set of jpeg images.\n",
    "\n",
    "    \"\"\"\n",
    "    images = convert_from_bytes(pdf_bytes.read())\n",
    "    return images\n",
    "\n",
    "\n",
    "def ocr_text(img, lang='eng'):\n",
    "    \"\"\"\n",
    "    Takes the text from image.\n",
    "    \n",
    "    :lang: language is `eng` by default,\n",
    "           use `eng+rus` for two languages in document\n",
    "\n",
    "    \"\"\"\n",
    "    text = str(pytesseract.image_to_string(\n",
    "        img,\n",
    "        lang=lang\n",
    "    ))\n",
    "    return text\n",
    "\n",
    "\n",
    "def ocr_text_dir(img_dir, lang='eng'):\n",
    "    \"\"\"\n",
    "    Takes the text from images in a folder.\n",
    "\n",
    "    \"\"\"\n",
    "    text = ''\n",
    "    for img_name in tqdm(sorted(os.listdir(img_dir))):\n",
    "        if '.jpg' in img_name:\n",
    "            img = Image.open(f'{IMG_PATH}/{img_name}')\n",
    "            text_tmp = ocr_text(img, lang=lang)\n",
    "            text = ' '.join([text, text_tmp])\n",
    "    return text\n",
    "\n",
    "\n",
    "st.header('Demo of image uploading', divider='rainbow')\n",
    "st.subheader('Uploading file and extracting text from it')\n",
    "st.divider()\n",
    "\n",
    "st.write('#### Upload you file or image')\n",
    "uploaded_file = st.file_uploader('Select a file (JPEG or PDF)')\n",
    "if uploaded_file is not None:\n",
    "    file_name = uploaded_file.name\n",
    "    lang = st.selectbox(\n",
    "            'Select language to extract ',\n",
    "            ('eng', 'rus', 'eng+rus')\n",
    "        )\n",
    "    if '.jpg' in file_name:\n",
    "        with st.spinner('Please wait...'):\n",
    "            bytes_data = uploaded_file.read()\n",
    "            img = Image.open(io.BytesIO(bytes_data))\n",
    "            \n",
    "            # image caption model for uploaded image\n",
    "            text = ocr_text(img, lang=lang)\n",
    "            st.divider()\n",
    "            st.write('#### Text extracted')\n",
    "            st.write(text)\n",
    "    elif '.pdf' in file_name:\n",
    "        with st.spinner('Please wait...'):\n",
    "            imgs = pdf2img(uploaded_file)\n",
    "            text = ''\n",
    "            for img in imgs:\n",
    "                text_tmp = ocr_text(img, lang=lang)\n",
    "                text = ' '.join([text, text_tmp])\n",
    "            st.divider()\n",
    "            st.write('#### Text extracted')\n",
    "            st.write(text)\n",
    "    else:\n",
    "        st.error('File read error', icon='⚠️')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca128d06-5fe3-48ab-a6b7-6b6bea0ac856",
   "metadata": {},
   "source": [
    "### 3.4. Add some faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "900be527-4853-46a3-86e3-13c96e64ec7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stapp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile stapp.py\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# Use of DeepFace framework\n",
    "# for faces detection and recognotion\n",
    "\n",
    "import io\n",
    "import cv2\n",
    "import streamlit as st\n",
    "from PIL import Image\n",
    "from deepface import DeepFace\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def zeroshot(classifier, classes, img):\n",
    "    scores = classifier(\n",
    "        img,\n",
    "        candidate_labels=classes\n",
    "    )\n",
    "    return scores\n",
    "\n",
    "\n",
    "with st.spinner('Please wait, application is initializing...'):\n",
    "    MODEL_ZERO_NAME = 'openai/clip-vit-base-patch16'\n",
    "    CLASSIFIER_ZERO = pipeline('zero-shot-image-classification', model=MODEL_ZERO_NAME)\n",
    "    CLASSES = [\n",
    "        'a photo of nature',\n",
    "        'a photo of cat',\n",
    "        'a photo of a party',\n",
    "        'a photo of a food'\n",
    "    ]\n",
    "    DEEPFACE_MODELS = [\n",
    "        'VGG-Face',\n",
    "        'Facenet',\n",
    "        'Facenet512',\n",
    "        'OpenFace',\n",
    "        'DeepFace',\n",
    "        'DeepID',\n",
    "        'ArcFace',\n",
    "        'Dlib',\n",
    "        'SFace',\n",
    "        'GhostFaceNet'\n",
    "    ]\n",
    "    DB_PATH = '/home/jovyan/dlba/topic_09/dlba_course_miba_25/app/data/db'\n",
    "\n",
    "st.header('Demo of image uploading', divider='rainbow')\n",
    "st.subheader('Uploading file and classifying it with zero-shot and face recognition')\n",
    "st.divider()\n",
    "\n",
    "st.write('#### Upload you image')\n",
    "uploaded_file = st.file_uploader('Select an image file (JPEG format)')\n",
    "if uploaded_file is not None:\n",
    "    file_name = uploaded_file.name\n",
    "    if '.jpg' in file_name:\n",
    "        with st.spinner('Please wait...'):\n",
    "            bytes_data = uploaded_file.read()\n",
    "            img = Image.open(io.BytesIO(bytes_data))\n",
    "            \n",
    "            # classifying image with zero-shot modele\n",
    "            scores = zeroshot(\n",
    "                classifier=CLASSIFIER_ZERO, \n",
    "                classes=CLASSES, \n",
    "                img=img\n",
    "            )\n",
    "            st.divider()\n",
    "            st.image(img, caption='zero-shot classification')\n",
    "            \n",
    "            # plot a diagram ith scores and scores output\n",
    "            st.divider()\n",
    "            df = pd.DataFrame(scores)\n",
    "            df = df.set_index('label')\n",
    "            st.bar_chart(df)\n",
    "            st.divider()\n",
    "            st.caption('Scores dictionary:')\n",
    "            st.write(scores)\n",
    "            \n",
    "            # faces detection and recognition\n",
    "            results = DeepFace.find(\n",
    "                img_path=np.array(img),  # face to find\n",
    "                db_path=f'{DB_PATH}',  # path to directory with faces\n",
    "                model_name=DEEPFACE_MODELS[0],\n",
    "                enforce_detection=False\n",
    "            )\n",
    "            st.divider()\n",
    "            st.caption('Faces recognition:')\n",
    "            found = []\n",
    "            for result in results:\n",
    "                if len(result):\n",
    "                    name = result.identity.values\n",
    "                    if name:\n",
    "                        found.append(\n",
    "                            name[0].replace(f'{DB_PATH}/', '').replace('.jpg', '')\n",
    "                        )\n",
    "            if found:\n",
    "                st.write(f'Found: {\" ,\".join(found)}')\n",
    "            else:\n",
    "                st.write('No known faces found')\n",
    "    else:\n",
    "        st.error('File read error', icon='⚠️')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d45be59-3147-4116-b8f0-9ab4555ea8a7",
   "metadata": {},
   "source": [
    "## 4. Move to developing app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fd486b-d1a3-4f75-a181-372e0ee2e9b7",
   "metadata": {},
   "source": [
    "Let's get out Jupyter notebooks to a hardcore development process..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119322e9-8ec7-46c3-9352-1a803f1eadb8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <font color='red'>HOME ASSIGNMENT (Final project)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb81d902-323d-4dc5-997e-420d234b2e04",
   "metadata": {},
   "source": [
    "Your final project tasks are:\n",
    "1. Enlarge number of categories for uploaded images classification\n",
    "2. Change YOLO detection to segmentation model (use new version of YOLO, version 11 is released)\n",
    "3. Apply emotion detector model and add emotions for your friends faces recognition pipeline\n",
    "4. Implement application's page for OCR with Tesseract library\n",
    "5. Add text summarization option for text extracted in OCR page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495287f6-5d38-4ecf-9c81-2afe33fa639b",
   "metadata": {},
   "source": [
    "<font color='red'>Use application code snippets for the Final project, not the notebooks!</font> To run application (1) move to `app` directory (2) use the following command from the terminal:\n",
    "\n",
    "`streamlit run app/Main_page.py --server.port 20000 --browser.gatherUsageStats False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "09bafb2b-4e54-4acf-b672-be7c918d48a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"classes\": \n",
    "          [\"a photo of nature\", \"a photo of cat\", \"a photo of a party\", \"a photo of a food\", \"a photo of transport\",\n",
    "          \"a photo of book\", \"a photo of window\", \"a photo of sports\", \"a photo of art\", \"a photo of travel\", \"a photo of person\",\n",
    "          \"a photo of headphones\", \"a photo of architecture\", \"a photo of a concert\"], \n",
    "          \"db_dict\": {\n",
    "                \"a photo of nature\": \"nature\",\n",
    "                \"a photo of cat\": \"cats\",\n",
    "                \"a photo of a party\": \"party\",\n",
    "                \"a photo of a food\": \"food\",\n",
    "                \"a photo of transport\": \"transport\",\n",
    "                \"a photo of book\": \"books\",\n",
    "                \"a photo of window\": \"windows\",\n",
    "                \"a photo of sports\": \"sports\",\n",
    "                \"a photo of art\": \"art\",\n",
    "                \"a photo of travel\": \"travel\",\n",
    "                \"a photo of person\": \"person\",\n",
    "                \"a photo of headphones\" : \"headphones\",\n",
    "                \"a photo of architecture\" : \"architecture\",\n",
    "                \"a photo of a concert\" : \"concert\"\n",
    "                },\n",
    "                      \"th_others\": 0.5, \n",
    "                      \"imgs_path\": \"/home/jovyan/dlba/dlba_course_miba_25/topic_09/app/data/\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "91c59863-3864-4d1b-8caa-87f8cbfd3ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('./app/config.json', 'w') as file:\n",
    "    json.dump(config, file, indent = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4c5a6515-65a2-4a40-99c8-4ad9259680ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "images = {\n",
    "    'https://www.shutterstock.com/editorial/image-editorial/MeTdM857MdTfgfzbMTYyMjg=/ana-de-armas-attending-louis-vuitton-womenswear-440nw-13795334du.jpg' : 'ana_de_armas1',\n",
    "    'https://hips.hearstapps.com/hmg-prod/images/ana-de-armas-1626771511.jpg' : 'ana_de_armas2',\n",
    "    'https://assets.vogue.com/photos/5f060d7413f92cad8bc1c8da/master/pass/unnamed-1.jpg' : 'daisy_jones',\n",
    "    'https://parade.com/.image/w_3840,q_auto:good,c_limit/MTkzMjg5MDc1ODMzMTg1ODU5/anne-hathaway-net-worth-ftr.jpg' : 'anne_hathaway',\n",
    "    'https://www.crash.fr/wp-content/uploads/2021/10/dior_anyataylorjoy_sami-darsin-1067x1600.jpg' : 'joy',\n",
    "    'https://es.web.img3.acsta.net/pictures/16/05/20/13/01/555938.jpg' : 'elle_fanning',\n",
    "    'https://m.media-amazon.com/images/M/MV5BMTc1NDUzMzM5MV5BMl5BanBnXkFtZTcwNzY1OTYyOQ@@._V1_.jpg' : 'cassel',\n",
    "    'https://hips.hearstapps.com/hmg-prod/images/paul-mescal-attends-the-soho-house-awards-at-dumbo-house-on-news-photo-1719868300.jpg?crop=0.876xw:0.585xh;0.0731xw,0.0420xh&resize=640:*' : 'paul_mescal'\n",
    "\n",
    "}\n",
    "images_path = '/home/jovyan/dlba/dlba_course_miba_25/topic_09/app/data/db'\n",
    "for image in images.items():\n",
    "    image_url = image[0]\n",
    "    image_title = image[1]\n",
    "    pic = Image.open(\n",
    "        requests.get(image_url, stream=True).raw\n",
    "    ).convert('RGB')\n",
    "    pic.save(f'{images_path}/{image_title}.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd8fd2f5-46e1-4312-97c2-2ba9d647063a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1785fff9-391e-4e78-b44e-f185f63891f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('huggingface_token.txt', 'r') as f:\n",
    "    token = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28dcfcc1-a550-42e6-868f-1fb68292a768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e04fedb997774d77ab8d0a65dccf1b23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/46.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09dcb23c839e460c82b4fe335f0e3d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67fea09a42044a3cb1abe22b36bbd9f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/34.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "403eeac3d7354e0cb2318e4bf633429e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49419167c27c4a2687f6099690f412e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 15:26:21.186660: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-29 15:26:21.237049: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab0a333bbab4b7f8ecfb2357c3f39ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b15ff84f661f4f01b5de1cb81324aa6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/156 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"google/t5gemma-b-b-prefixlm\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4d3fcf1a-620b-442a-8ff7-2ec7307c97ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Bangalore is the capital and the largest city of the Indian state of Karnataka. It has a population of more than 8 million and a metropolitan population of around 11 million, making it the third most populous city and fifth most populous urban agglomeration in India. Located in southern India on the Deccan Plateau, at a height of over 900 m (3,000 ft) above sea level, Bangalore is known for its pleasant climate throughout the year. Its elevation is the highest among the major cities of India.The city's history dates back to around 890 CE, in a stone inscription found at the Nageshwara Temple in Begur, Bangalore. The Begur inscription is written in Halegannada (ancient Kannada), mentions 'Bengaluru Kalaga' (battle of Bengaluru). It was a significant turning point in the history of Bangalore as it bears the earliest reference to the name 'Bengaluru'. In 1537 CE, Kempé Gowdā – a feudal ruler under the Vijayanagara Empire – established a mud fort considered to be the foundation of modern Bangalore and its oldest areas, or petes, which exist to the present day.\n",
    "After the fall of Vijayanagar empire in 16th century, the Mughals sold Bangalore to Chikkadevaraja Wodeyar (1673–1704), the then ruler of the Kingdom of Mysore for three lakh rupees. When Haider Ali seized control of the Kingdom of Mysore, the administration of Bangalore passed into his hands. \n",
    "The city  was captured by the British East India Company after victory in the Fourth Anglo-Mysore War (1799), who returned administrative control of the city to the Maharaja of Mysore. The old city developed in the dominions of the Maharaja of Mysore and was made capital of the Princely State of Mysore, which existed as a nominally sovereign entity of the British Raj. In 1809, the British shifted their cantonment to Bangalore, outside the old city, and a town grew up around it, which was governed as part of British India. Following India's independence in 1947, Bangalore became the capital of Mysore State, and remained capital when the new Indian state of Karnataka was formed in 1956. The two urban settlements of Bangalore – city and cantonment – which had developed as independent entities merged into a single urban centre in 1949. The existing Kannada name, Bengalūru, was declared the official name of the city in 2006.\n",
    "Bangalore is widely regarded as the \"Silicon Valley of India\" (or \"IT capital of India\") because of its role as the nation's leading information technology (IT) exporter. Indian technological organisations are headquartered in the city. A demographically diverse city, Bangalore is the second fastest-growing major metropolis in India. Recent estimates of the metro economy of its urban area have ranked Bangalore either the fourth- or fifth-most productive metro area of India. As of 2017, Bangalore was home to 7,700 millionaires and 8 billionaires with a total wealth of $320 billion. It is home to many educational and research institutions. Numerous state-owned aerospace and defence organisations are located in the city. The city also houses the Kannada film industry. It was ranked the most liveable Indian city with a population of over a million under the Ease of Living Index 2020.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0bceb224-f8ae-44fb-ac40-0d1b2a3f90b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text.split()) * 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d4f6ae65-3926-4c09-9cab-b8696fb9a38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Суммаризация: The city has a population of around 8 million and a metropolitan population of around 11 million, making it the third most populous city and fifth most populous urban agglomeration in India. It is home to many educational and research institutions. Numerous state-owned aerospace and defence organisations are located in the city. The city also houses the Kannada film industry. It was ranked the most liveable Indian city with a population of over a million under the Ease of Living Index 2020. As of 2020, Bangalore was home to 7,700 millionaires and \n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"google/t5gemma-b-b-prefixlm\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    ")\n",
    "\n",
    "prompt = f\"\"\"\"Summarize text in 2-3 short sentences {text}\"\"\"\n",
    "outputs = pipe(\n",
    "    prompt,\n",
    "    max_new_tokens= 120,           # ⬅️ Ограничиваем длину\n",
    "    min_new_tokens=30,            # ⬅️ Минимальная длина\n",
    "    repetition_penalty=2.5,       # ⬅️ Штраф за повторения\n",
    "    length_penalty= 1.9,           # ⬅️ Поощряет краткость\n",
    "    num_beams=4,                  # ⬅️ Улучшает качество\n",
    "    early_stopping=True,          # ⬅️ Останавливается когда готово\n",
    "    temperature=0.1,              # ⬅️ Контроль случайности\n",
    "    do_sample=True,                # ⬅️ Включает сэмплирование\n",
    ")\n",
    "summary = outputs[0][\"generated_text\"]\n",
    "print(\"📝 Суммаризация:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f790d2ce-8602-4f27-a230-85de051d5d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (773 > 512). Running this sequence through the model will result in indexing errors\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Оригинал: 525 слов\n",
      "📝 Суммаризация: Bangalore has a population of more than 8 million and a metropolitan population of around 11 million . It is the third most populous city in the Indian state of Karnataka . The city's history dates back to 890 CE, in a stone inscription found at the Nageshwara Temple . \n",
      " 50 слов\n",
      "📊 Сжатие: 90.5%\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    model=\"Falconsai/text_summarization\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "summary = summarizer(\n",
    "    text,\n",
    "    max_length=150,\n",
    "    min_length=40,\n",
    "    do_sample=False\n",
    ")[0]['summary_text']\n",
    "\n",
    "print(\"📄 Оригинал:\", len(text.split()), \"слов\")\n",
    "print(\"📝 Суммаризация:\", summary, '\\n', len(summary.split()), \"слов\")\n",
    "print(\"📊 Сжатие:\", f\"{(1 - len(summary.split()) / len(text.split())) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a9d44a-fe29-4efc-ac5c-e0c90f8d6a2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
